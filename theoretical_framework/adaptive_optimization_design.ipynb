{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. General problem statement\n",
    "\n",
    "### What do we look for?\n",
    "\n",
    "We look for the optimal design $d^*$ such as:\n",
    "\\begin{equation}\n",
    " d^* = \\arg \\max_{d}{U(d)}\n",
    "\\end{equation}\n",
    "with $U(d)$, a function that returns the 'utility' of the design in terms of information regarding the estimation of parameters OR model comparison.\n",
    "\n",
    "\n",
    "### Notation / Core entities\n",
    "\n",
    "$\\mathcal{D}$: the design space. It can be discreet or continuous depending on the case of application. It can also be multi dimensional. We can also think of it as the 'query space'.\n",
    "\n",
    "Example: in the case of the 'exponential decay' model, it is the delay between the initial presentation and the recall testing (for instance, a number of seconds).\n",
    "\n",
    "$\\mathcal{M}$: the model space. It is discreet.\n",
    "\n",
    "Example: considering memory models, a possible model space is $\\{POW, EXP\\}$, with $POW$ a power law model of forgetting, such that the probability of recall is:\n",
    "\\begin{equation}\n",
    "p(\\delta) = \\alpha \\delta^{- \\beta}\n",
    "\\end{equation}\n",
    "\n",
    "and $EXP$, a exponential decay of forgetting, such that the probability of recall after a delay $\\delta$ is:\n",
    "\\begin{equation}\n",
    "p(\\delta) = \\alpha e^{-\\beta \\delta}\n",
    "\\end{equation}\n",
    "with $\\alpha$ and $\\beta$, the free parameters.\n",
    "\n",
    "$\\Theta_m$: the parameter space for the model $m \\in \\mathcal{M}$. It is $x$-dimensional, $x$ being the number of free parameters of the model $m$.\n",
    "\n",
    "$Y$: the observation space. It can be discreet of continuous space. It could be multi-dimensional. \n",
    "\n",
    "Example: considering memory models, it can be $\\{0, 1\\}$, with $1$, a successful recall, and $0$ a missed recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parameter inference\n",
    "\n",
    "### Define the utility function\n",
    "\n",
    "Based on Cavagnaro (2009) https://pubmed.ncbi.nlm.nih.gov/20028226/ and Myung (2013) https://pubmed.ncbi.nlm.nih.gov/23997275/, we define a utility function $U$ as the mutual information of $\\Theta$ and $Y\\mid d$:\n",
    "\\begin{equation}\n",
    "    U(d) = I(\\Theta; Y\\mid d)\n",
    "\\end{equation}\n",
    "Y being the possible observations (e.g., behavioral outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative definitions\n",
    "\n",
    "1. It can be based on the Kullbackâ€“Leibler divergence between the posterior and the prior. For instance, following Ouyang et al. (2016) https://arxiv.org/pdf/1608.05046.pdf:\n",
    "\n",
    "\\begin{equation}\n",
    "U(d) = \\mathbb{E}_{p(y \\mid d)} D_{KL}(P(\\Theta | d, y), P(\\Theta)) \n",
    "\\end{equation}\n",
    "\n",
    "Something in the same line of thought could be\n",
    "\\begin{equation}\n",
    "U(d) = \\int p(y \\mid d) D_{KL}(P(\\Theta | d, y), P(\\Theta)) d y\n",
    "\\end{equation}\n",
    "Note: Formulation here probably needs correction/adpatation.\n",
    "\n",
    "\n",
    "2. It can be based only on the entropy of the posterior:\n",
    "\n",
    "\\begin{equation}\n",
    "U(d) = \\int p(y \\mid d) H(\\Theta \\mid d, y) d y \n",
    "\\end{equation}\n",
    "Note: Formulation here probably needs correction/adpatation.\n",
    "\n",
    "For example: Candela et al. (2018) https://www.ri.cmu.edu/publications/automatic-experimental-design-using-deep-generative-models-of-orbital-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian revision of belief regarding the parameters\n",
    "Supposing that an experiment was carried out with design $d^*$, and an outcome $y_t$ was observed, the prior over $\\theta \\in \\Theta$ at $t$, $p_t(\\theta)$, is revised the following way:\n",
    "\\begin{equation}\n",
    "     p_{t+1}(\\theta) = \\dfrac{\n",
    "      p(y_t \\mid \\theta, d^*) p_t(\\theta)\n",
    "     }{\n",
    "        \\int p(y_t \\mid \\theta, d^*) p_t(\\theta) d\\theta\n",
    "     }\n",
    "\\end{equation}\n",
    "with $y_t \\in Y$, the observation at $t$, $d^*\\in \\mathcal{D}$, the design chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exchanging definitions\n",
    "\n",
    "Still based Myung et al. (2013), a way to define $U$ in case of parameter estimation is:\n",
    "\n",
    "\\begin{equation}\n",
    "U(d) = \\int \\int u(d, \\theta, y) p(y \\mid \\theta, d) p(\\theta) d\\theta dy\n",
    "\\end{equation}\n",
    "if $U(d) = I(\\Theta; Y\\mid d)$ then $u(d, \\theta, y) = \\log \\left( \\frac{p (\\theta \\mid y, d)}{p(\\theta)} \\right)$\n",
    "which means\n",
    "\n",
    "\\begin{equation}\n",
    "U(d) = I(\\Theta; Y\\mid d) = \\int \\int \\log \\left( \\frac{p (\\theta \\mid y, d)}{p(\\theta)} \\right) p(y \\mid \\theta, d) p(\\theta) d\\theta dy\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "U(d) &= I(\\Theta; Y\\mid d) \\\\\n",
    "&= H(\\Theta) - H(\\Theta \\mid Y, d)\\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "H(\\Theta) &= - \\int p(\\theta) \\log p(\\theta) d\\theta \\\\\n",
    "&= - \\int p(\\theta) \\log p(\\theta) \\int p(y \\mid \\theta, d) dy d\\theta \\\\\n",
    "&= - \\int \\int p(\\theta) \\log p(\\theta) p(y \\mid \\theta, d) dy d\\theta \\\\\n",
    "&= - \\int \\int p(\\theta) p(y \\mid \\theta, d) \\log p(\\theta) dy d\\theta \\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "H(\\Theta \\mid Y, d) &= - \\int p(y \\mid d) \\int p( \\theta \\mid y, d) \\log p(\\theta \\mid y, d) dy d\\theta \\\\\n",
    "&=  - \\int p(y \\mid d) \\int p(\\theta) p( y \\mid \\theta, d) \\frac{1}{p(y \\mid d)}   \\log p(\\theta \\mid y, d) dy d\\theta \\\\\n",
    "&=  - \\int \\int p(y \\mid d) p(\\theta) p( y \\mid \\theta, d) \\frac{1}{p(y \\mid d)}   \\log p(\\theta \\mid y, d) dy d\\theta \\\\\n",
    "&= - \\int \\int p(\\theta) p( y \\mid \\theta, d)   \\log p(\\theta \\mid y, d) dy d\\theta \\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "U(d) &= \\int \\int - p(\\theta) \\log p(\\theta) p(y \\mid \\theta, d) + p(\\theta) p( y \\mid \\theta, d)   \\log p(\\theta \\mid y, d) dy d\\theta \\\\\n",
    "&= \\int \\int - p(\\theta) p(y \\mid \\theta, d) \\log p(\\theta) + p(\\theta) p( y \\mid \\theta, d)   \\log p(\\theta \\mid y, d) dy d\\theta \\\\\n",
    "&= \\int \\int p(\\theta) p(y \\mid \\theta, d) [ \\log p (\\theta \\mid y, d) - \\log p(\\theta) ] dy d\\theta \\\\\n",
    "&= \\int \\int p(\\theta) p(y \\mid \\theta, d) \\log \\frac{p (\\theta \\mid y, d)}{p(\\theta)} dy d\\theta \\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Implementation follows what has been done in: https://github.com/adopy/adopy\n",
    "\n",
    "Actually, in the implementation, we can use the fact that the mutual information is symetric. Indeed, for any pair of random variables $X$ and $Y$, we have:\n",
    "\n",
    "\\begin{equation}\n",
    "I(X; Y) = H(X) - H(X \\mid Y) = I(Y; X) = H(Y) - H(Y \\mid X)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means that in our case, we have:\n",
    "\\begin{equation}\n",
    "I(\\Theta; Y \\mid d) = I(Y \\mid d; \\Theta) = H(Y \\mid d) - H(Y \\mid \\Theta, d) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The marginal entropy is:\n",
    "\n",
    "\\begin{equation}\n",
    "   H(Y \\mid d) = - \\int p(y \\mid d) \\log p(y \\mid d) dy\n",
    "\\end{equation}\n",
    "\n",
    "In our implementation, we use log probabilities instead of proabilities, and $Y$ is a discreet space:\n",
    "\\begin{equation}\n",
    "   H(Y \\mid d) =  - \\sum_{y \\in Y} \\exp[\\log p(y \\mid d)] \\log p(y \\mid d)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional entropy of $Y$ given the outcome random variable $\\Theta$ and design $d$ is:\n",
    "\\begin{equation}\n",
    "        H(Y \\mid \\Theta, d) = - \\int p(\\theta) \\int p(y\\mid \\theta, d) \\log p(y\\mid \\theta, d) dyd\\theta\n",
    "\\end{equation}\n",
    "\n",
    "In our implementation, we use log probabilities instead of proabilities, and $Y$ is a discreet space, and since we use a grid exploration technique, $\\Theta$ is also considered to be a discreet space:\n",
    "\\begin{equation}\n",
    "H(Y \\mid \\Theta, d) = - \\sum_{\\theta \\in \\Theta} \\exp[\\log p(\\theta)] \\sum_{y\\in Y} \\exp[ \\log p(y\\mid \\theta, d)] \\log p(y\\mid \\theta, d)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model comparison\n",
    "\n",
    "In case of model comparison, still following Cavagnaro (2009) and Myung et al. (2013), we need to modify our definition of $U$:\n",
    "\n",
    "\\begin{equation}\n",
    "     U(d) = \\sum_m p(m) \\int \\int u(d, \\theta_m, y_m) p(y_m \\mid \\theta_m, d) p_{\\theta_m} dy_m d\\theta_m,\n",
    "\\end{equation}\n",
    "where m = {1, 2, ..., K} is one of a set of K models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "U(d) &= I(M; Y\\mid d) \\\\\n",
    "u(d, \\theta_m, y_m) &= \\log \\frac{p(m \\mid y, d)}{p(m)}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where $I(M; Y |d)$ is the mutual information between the model random variable M and the outcome random variable conditional upon design d, Y |d. \n",
    "\n",
    "$p(m \\mid y, d)$ is the posterior model probability of model $m$ obtained by Bayes rule as:\n",
    "\\begin{equation}\n",
    "p(m|y, d) = \\frac{p(y|m, d)p(m)}{p(y|d)} \n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\\begin{equation}\n",
    "p(y|m, d) = \\int p(y| \\theta_m, d)p(\\theta_m) d\\theta_m \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p(y|d) = \\sum_m p(y|m, d)p(m)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian update of belief regarding the model\n",
    "\n",
    "Cavagnaro (2009) and Myung et al. (2013) propose to use:\n",
    "\\begin{equation}\n",
    "p_{t+1}(m) = \\frac{p_1(m)}{\\sum_{k=1}^K p_1(k) BF_{(k, m)}(y_t \\mid d^*)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "BF(k,m) (y \\mid d) = \\frac{\\int p(y \\mid \\theta_k, d) p(\\theta_k) d\\theta_k}{\\int p(y \\mid \\theta_m, d) p(\\theta_m) d\\theta_m} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the fact that $p_1(m)$ is used instead of $p_t(m)$, which confers a lot of influence to the initial prior (i.e. the prior before beginning the experience)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alernative possibility would be to use:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "p_{t+1}(m) &= \\frac{p_t(m)p(y_t \\mid m, d^*)}{\\sum_{k=1}^{K} p(k) p_t(k) p(y_t \\mid m, d^*)}\\\\\n",
    "&= \\frac{p_t(m) \\int p(y_t \\mid \\theta_m, d^*) p(\\theta_m) d\\theta_m}{\\sum_{k=1}^{K} p(k) \\int p(y_t \\mid \\theta_k, d^*) p(\\theta_k) d\\theta_k}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "For the implementation, we need to work with log probabilities instead of probabilities (to avoid as much as possible errors due to the floating point precision) \n",
    "\\begin{equation}\n",
    "\\log p_{t+1}(m) = \\log p_1(m) - \\log \\sum_{k=1}^K \\exp \\left( \\log p_1(k) + \\log \\sum_{\\theta \\in \\Theta_k} \\exp[\\log p(y\\mid \\theta, d) + \\log p (\\theta)]  - \\log \\sum_{\\theta \\in \\Theta_m} \\exp[\\log p(y\\mid \\theta, d) + \\log p (\\theta)]  \\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "U(d) = \\sum_{m \\in M} p(m) \\sum_{\\theta \\in \\Theta_m} \\sum_{y \\in Y} \\exp \\left( \n",
    "\\log \\left(\n",
    "\\log \\sum_{\\theta' \\in \\Theta_m} \\exp[ \n",
    "\\log p(y \\mid \\theta', d) + \\log p(\\theta')]\n",
    "- \\log \\sum_{m' in M} \\exp \\left( \\log p(m) +  \n",
    "\\log \\sum_{\\theta' \\in \\Theta_m} \\exp[ \n",
    "\\log p (y \\mid \\theta', d) + \\log p(\\theta')] \\right)\\right)\n",
    "+ \\log p(y \\mid \\theta, d) + \\log p(\\theta)\n",
    "\\right)\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
